{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fGt_onW449nN"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential\n",
    "import tqdm\n",
    "import sklearn.metrics\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "79FNzkltU8gr"
   },
   "outputs": [],
   "source": [
    "df= []\n",
    "dataset = open('spambase.data')\n",
    "reader = csv.reader(dataset)\n",
    "next(reader, None)\n",
    "\n",
    "for r in reader:\n",
    "    df.append(r)\n",
    "dataset.close()\n",
    "\n",
    "X = [x[:-1] for x in df]\n",
    "y = [x[-1] for x in df] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_G_UD7_GBR6-"
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "\n",
    "X = t.texts_to_sequences(X)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = pad_sequences(X, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Y7bZqzCcQoui"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
    "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
    "TEST_SIZE = 0.25 # ratio of testing set\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10 # number of epochs\n",
    "\n",
    "label2int = {\"ham\": 0, \"spam\": 1}\n",
    "int2label = {0: \"ham\", 1: \"spam\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WYy5N_Nc9XRM"
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Akytzlw6BL7K",
    "outputId": "027a55da-76e8-4162-964c-fb83a69c0ee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4600, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ynau_FMbBEK3"
   },
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7HjRhkABHSS",
    "outputId": "2f936790-e322-4034-f467-c704f09f3e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (3450, 100)\n",
      "X_test.shape: (1150, 100)\n",
      "y_train.shape: (3450, 2)\n",
      "y_test.shape: (1150, 2)\n"
     ]
    }
   ],
   "source": [
    "T_SIZE = 0.25\n",
    "# split and shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=T_SIZE, random_state=7)\n",
    "# print our data shapes\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kkiZjifkMCs3"
   },
   "outputs": [],
   "source": [
    "def get_embedding_vectors(dim=100):\n",
    "  embedding_index = {}\n",
    "\n",
    "  with open(f\"glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
    "        for l in tqdm.tqdm(f, \"Reading GloVe\"):\n",
    "            values = l.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = vectors\n",
    "\n",
    "  w_index = t.word_index\n",
    "\n",
    "  embedding_matrix = np.zeros((len(w_index)+1, 100))\n",
    "  for word, i in w_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400000it [00:20, 19086.73it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = get_embedding_vectors()\n",
    "models = Sequential()\n",
    "models.add(Embedding(4512,\n",
    "              EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "models.add(LSTM(128, recurrent_dropout=0.2))\n",
    "models.add(Dropout(0.3))\n",
    "models.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKhxrHybNPQH",
    "outputId": "c01c2281-3538-44d3-b372-c0d67249b8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          451200    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 568,706\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 451,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(models.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zk4ZbBEVN0LV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaE4S0xbN5pK"
   },
   "outputs": [],
   "source": [
    "optimum=optimizers.Adam(clipvalue=0.5)\n",
    "models.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUjH7J1kN8gj",
    "outputId": "c5c522e9-d651-4001-e297-d40cb46b535e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.7522\n",
      "Epoch 1: val_loss improved from inf to 0.37496, saving model to results/spam_classifier_0.37.h5\n",
      "54/54 [==============================] - 21s 331ms/step - loss: 0.5129 - accuracy: 0.7522 - val_loss: 0.3750 - val_accuracy: 0.8461\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.8432\n",
      "Epoch 2: val_loss did not improve from 0.37496\n",
      "54/54 [==============================] - 17s 323ms/step - loss: 0.3720 - accuracy: 0.8432 - val_loss: 0.4540 - val_accuracy: 0.7870\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8641\n",
      "Epoch 3: val_loss improved from 0.37496 to 0.32548, saving model to results/spam_classifier_0.33.h5\n",
      "54/54 [==============================] - 18s 334ms/step - loss: 0.3400 - accuracy: 0.8641 - val_loss: 0.3255 - val_accuracy: 0.8617\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.2991 - accuracy: 0.8814\n",
      "Epoch 4: val_loss improved from 0.32548 to 0.29979, saving model to results/spam_classifier_0.30.h5\n",
      "54/54 [==============================] - 17s 318ms/step - loss: 0.2991 - accuracy: 0.8814 - val_loss: 0.2998 - val_accuracy: 0.8800\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8806\n",
      "Epoch 5: val_loss did not improve from 0.29979\n",
      "54/54 [==============================] - 17s 324ms/step - loss: 0.2912 - accuracy: 0.8806 - val_loss: 0.3317 - val_accuracy: 0.8557\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.2752 - accuracy: 0.8904\n",
      "Epoch 6: val_loss improved from 0.29979 to 0.26415, saving model to results/spam_classifier_0.26.h5\n",
      "54/54 [==============================] - 18s 329ms/step - loss: 0.2752 - accuracy: 0.8904 - val_loss: 0.2642 - val_accuracy: 0.9070\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.8910\n",
      "Epoch 7: val_loss improved from 0.26415 to 0.26150, saving model to results/spam_classifier_0.26.h5\n",
      "54/54 [==============================] - 18s 330ms/step - loss: 0.2674 - accuracy: 0.8910 - val_loss: 0.2615 - val_accuracy: 0.8904\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.8983\n",
      "Epoch 8: val_loss did not improve from 0.26150\n",
      "54/54 [==============================] - 17s 324ms/step - loss: 0.2573 - accuracy: 0.8983 - val_loss: 0.2847 - val_accuracy: 0.8774\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9026\n",
      "Epoch 9: val_loss improved from 0.26150 to 0.24674, saving model to results/spam_classifier_0.25.h5\n",
      "54/54 [==============================] - 17s 323ms/step - loss: 0.2480 - accuracy: 0.9026 - val_loss: 0.2467 - val_accuracy: 0.9113\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.9046\n",
      "Epoch 10: val_loss did not improve from 0.24674\n",
      "54/54 [==============================] - 17s 316ms/step - loss: 0.2458 - accuracy: 0.9046 - val_loss: 0.2621 - val_accuracy: 0.8913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1682a71d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
    "# model checkpoint for saving best weights\n",
    "model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}.h5\", save_best_only=True,\n",
    "                                    verbose=1)\n",
    "# for better visualization\n",
    "tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
    "# train the model\n",
    "models.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          callbacks=[tensorboard, model_checkpoint],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6oz7tdLOACz",
    "outputId": "6e364f09-0529-4f6e-9adb-3b5087e6d6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 3s 73ms/step - loss: 0.2621 - accuracy: 0.8913\n",
      "[+] Accuracy: 89.13%\n"
     ]
    }
   ],
   "source": [
    "# get the loss and metrics\n",
    "r = models.evaluate(X_test, y_test)\n",
    "# extract those\n",
    "loss = r[0]\n",
    "accuracy = r[1]\n",
    "\n",
    "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiKJZBQUO8NY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Madhavi_deep_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
